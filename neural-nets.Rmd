---
title: "Neural Networks"
output: html_notebook
---

```{r}
install.packages("devtools")
devtools::install_github("rstudio/keras") # update all if prompted
library(keras)
install_keras() # install Miniconda if prompted
```

```{r}
library(keras)
install_keras()
```


```{r}
set.seed(2018)
n<-10000
ntrain = 6000
lw <- runif(n)
rw <- runif(n)
ld <- runif(n)
rd <- runif(n)
tilt <- as.factor(sign(rw*rd-lw*ld))
levels(tilt) <- c("N","P")
dat <- data.frame(tilt,lw,rw,ld,rd)
head(dat)
```


```{r}
x_train <- as.matrix(dat[1:ntrain,-1])
x_test <- as.matrix(dat[-(1:ntrain),-1])
y_train <- model.matrix(~tilt-1, data=dat[1:ntrain, ])
y_test <- model.matrix(~tilt-1, data=dat[-(1:ntrain), ])
```

Define FNN

```{r}
?layer_dense
```


```{r}
hidden_layer <- layer_dense(units = 50, activation = "relu", input_shape = c(4))
output_layer <- layer_dense(units = 2, activation = "softmax")
model <- keras_model_sequential(list(hidden_layer, output_layer))
summary(model)
```

Specify loss fct

```{r}
compile(model, optimizer="sgd", loss="categorical_crossentropy", metrics="acc")
```

Train model

```{r}
fit(model, x_train, y_train, epochs=20, batch_size=10)
```


```{r}
evaluate(model, x_test, y_test)
```


```{r}
hidden_layer <- layer_dense(units = 10, activation = "sigmoid", input_shape = c(4))
hidden_layer2 <- layer_dense(units = 10, activation = "sigmoid")
hidden_layer3 <- layer_dense(units = 10, activation = "sigmoid")
output_layer <- layer_dense(units = 2, activation = "softmax")
model <- keras_model_sequential(list(hidden_layer, hidden_layer2,hidden_layer3, output_layer))
wt0 <- get_weights(model) # store the initialised weights for later reference
compile(model, optimizer="sgd", loss="categorical_crossentropy", metrics="acc")
fit(model, x_train, y_train, batch_size=10, epochs=20)
evaluate(model, x_test, y_test)
```


```{r}
set_weights(model, wt0)
fit(model, x_train, y_train, batch_size=10, epochs=1)
t1 <- get_weights(model)
t1[[1]]-wt0[[1]] # compare weights of the first hidden laye
```

Fit instead with relu to avoid vanishing gradients

```{r}
hidden_layer <- layer_dense(units = 10, activation = "relu", input_shape = c(4))
hidden_layer2 <- layer_dense(units = 10, activation = "relu")
hidden_layer3 <- layer_dense(units = 10, activation = "relu")
output_layer <- layer_dense(units = 2, activation = "softmax")
model <- keras_model_sequential(list(hidden_layer, hidden_layer2,hidden_layer3, output_layer))
wt0 <- get_weights(model) # store the initialised weights for later reference
compile(model, optimizer="sgd", loss="categorical_crossentropy", metrics="acc")
fit(model, x_train, y_train, batch_size=10, epochs=20)
evaluate(model, x_test, y_test)
```

------------------

MNIST

```{r}
filePath <- "https://raw.githubusercontent.com/AJCoca/SLP19/master/"
fileName <- "mnist.csv"
mnist <- read.csv(paste0(filePath, fileName), header = TRUE)
x_train <- as.matrix(mnist[1:4000,-1])
y_train <- mnist[1:4000,1]
x_test <- as.matrix(mnist[4001:6000,-1])
y_test <- mnist[4001:6000,1]
```

```{r}
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- as.factor(y_train)
y_test <- as.factor(y_test)
y_train <- model.matrix(~y_train-1)
y_test <- model.matrix(~y_test-1)
```

```{r}
dim(x_train)
```


Fit a dense FFN

```{r}
hidden_layer <- layer_dense(units=256, activation = 'relu', input_shape = c(784))
output_layer <- layer_dense(units=10, activation = 'softmax')
model_mnist <- keras_model_sequential(list(hidden_layer, output_layer))
summary(model_mnist)
```


```{r}
wts <- get_weights(model_mnist)
str(wts)
```

```{r}
compile(model_mnist, optimizer="sgd", loss="categorical_crossentropy", metrics="acc")
```

```{r}
fit(model_mnist, x_train, y_train, epochs=10, batch_size=10)
evaluate(model_mnist, x_test, y_test)
```

```{r}
wt1 <- get_weights(model_mnist)
sum(wt1[[1]]!=wts[[1]])
```

Convolutional neural networks

```{r}
x_train <- array_reshape(x_train, dim=c(4000,28,28,1))
x_test <- array_reshape(x_test, dim=c(2000,28,28,1))
architecture <- list(
layer_conv_2d(filters=32, kernel_size=c(3,3), activation = "relu",
input_shape = c(28,28,1)),
layer_dropout(rate=0.4),
layer_conv_2d(filters=32, kernel_size=c(3,3), activation = "relu"),
layer_dropout(rate=0.4),
layer_flatten(),
layer_dense(units = 10, activation = "softmax")
)
model <- keras_model_sequential(architecture)
summary(model)
```

```{r}
compile(model, optimizer = "adam", loss="categorical_crossentropy", metrics="acc")
fit(model, x_train, y_train, epochs = 10, batch_size = 10)
evaluate(model, x_test, y_test)
```












